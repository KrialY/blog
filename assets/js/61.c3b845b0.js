(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{438:function(_,t,i){"use strict";i.r(t);var v=i(33),a=Object(v.a)({},(function(){var _=this,t=_.$createElement,i=_._self._c||t;return i("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[i("p",[_._v("神经网络与深度学习")]),_._v(" "),i("ul",[i("li",[i("h2",{attrs:{id:"深度学习、机器学习和人工智能"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#深度学习、机器学习和人工智能"}},[_._v("#")]),_._v(" 深度学习、机器学习和人工智能")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("关系")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/18f12b80-45bd-4dac-96ac-0385030f0f32-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"线性代数方面的知识"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#线性代数方面的知识"}},[_._v("#")]),_._v(" 线性代数方面的知识")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("基本概念与基础计算")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("空间向量")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/20a122b8-d770-4596-ac06-853f65d054ec-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("矩阵")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/245377f9-a193-4c33-93c8-59ea79a4449f-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("内积")]),_._v(" "),i("ul",[i("li",[_._v("向量的内积即向量的数量积，各位相乘然后相加即可，结果并非向量")]),_._v(" "),i("li",[_._v("例子\n"),i("ul",[i("li",[_._v("a=(1,2,3),b=(2,3,4).a内积b=1*2+2*3+3*4=2+6+12=20")])])])])]),_._v(" "),i("li",[i("p",[_._v("范数")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/b4dd359e-5a06-4691-b2df-071b04b90d81-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[i("p",[_._v("例子")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/012dc0f8-dd35-46a8-ac4b-2708892de25e-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("特征值与特征向量")]),_._v(" "),i("ul",[i("li",[i("img",{attrs:{src:"https://img.mubu.com/document_image/2cf5f494-e12d-4cb8-8964-3a5b17598512-7720513.jpg",alt:""}})])])]),_._v(" "),i("li",[i("p",[_._v("逆矩阵")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("用伴随矩阵")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/86e3e2fe-edb9-4cf8-b1e6-0f875ec15e71-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("初等变换")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3f74b100-9f52-4582-aaf4-4c7b33a7a96f-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("正定矩阵与半正定矩阵")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/6d0b91ec-f04a-456e-abb8-d5abd85f221b-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("相关题目")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3e304fe6-e12b-4d37-a4ca-a95144722f5e-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/e8765469-38e8-4a97-9bbd-564572f92305-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("Pytorch相关的函数计算")]),_._v(" "),i("ul",[i("li",[i("p",[i("a",{attrs:{href:"http://Torch.mm",target:"_blank",rel:"noopener noreferrer"}},[_._v("Torch.mm"),i("OutboundLink")],1),_._v("() and torch.matmul()")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/ca5377b4-cbf3-4ca5-837a-7096428197fe-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/0dcc53e1-41fa-4a8a-bf06-4471fb34fb04-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("理解与计算")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("链式求导法则")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/dfbf6487-8a23-4cd7-9f93-c4d1c4b38248-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("梯度：多元函数的一阶导数")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("概念")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3735d66c-9c67-44a2-b3b3-19445bcc6dd7-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("符号")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/0a0a5660-8a14-4c9a-9046-9d9432fc50f3-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("求多元函数的梯度")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("举个栗子🌰")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/73c1359d-7e20-4f17-8ef1-40d429d3004c-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/9a68884f-ae5d-4ef8-927f-5e5504b389c5-7720513.jpg",alt:""}})])])])])])]),_._v(" "),i("li",[i("p",[_._v("Hessian矩阵")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("概念")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/63bf8909-bd35-43c9-8923-6eb83f76d276-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("符号")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/5c48b77f-8ba4-4c0f-907a-b8feecde36f1-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("求多元函数的Hessian矩阵")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/b8d01675-3814-406f-a908-6f6899422c0d-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[i("p",[_._v("举个栗子🌰")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/7a7d23de-4ee0-4503-9cb9-0a7722895cd3-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/9a68884f-ae5d-4ef8-927f-5e5504b389c5-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/9b11cba5-7d99-47dc-9929-c36f1b7c44ad-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/04537bce-4989-444c-be54-0dd1b0ddc51c-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("意义")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("几何意义")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/40ed3624-4b29-40ea-839e-628f2ce83f7d-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("对于二次函数，其Hessian矩阵的特征值和特征向量的意义是什么？")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/0e8fce99-ad3c-4fe2-972e-ebff9a349cf5-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[_._v("利用Hessian矩阵点特征值来判断驻点为极大（小）值点还是鞍点")])])])])]),_._v(" "),i("li",[i("p",[_._v("小结")]),_._v(" "),i("ul",[i("li",[_._v("如果Hessian矩阵的所有特征值都为正（负），则函数存在一个唯一的强极小（大）点")]),_._v(" "),i("li",[_._v("如果特征值有正有负，则函数存在一个唯一的鞍点")]),_._v(" "),i("li",[_._v("如果所有的特征值都非负（正），但某些特征值为0，则函数存在一个弱极小（大）点，或没有驻点")])])])])]),_._v(" "),i("li",[i("p",[_._v("方向导数")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("定义")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/952d33d2-6e4c-4f5a-8ce9-9f079e1645c8-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("计算")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("举个栗子🌰")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/05b22f63-b3ca-436d-a0d7-7aa36d8e811f-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("哪个方向的斜率最大？")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/1912ad07-010e-4a29-931e-11dac88a927c-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("多元函数的泰勒级数的展开")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3b12f2bc-9771-4c46-a679-e2d6fcd18034-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[_._v("一阶展开")]),_._v(" "),i("li",[_._v("二阶展开")])])]),_._v(" "),i("li",[i("p",[_._v("最优点的必要条件")]),_._v(" "),i("ul",[i("li",[_._v("一阶\n"),i("ul",[i("li",[_._v("该点是驻点")])])]),_._v(" "),i("li",[_._v("二阶\n"),i("ul",[i("li",[_._v("Hessian矩阵是半正定的")])])])])])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"优化算法-梯度下降算法"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#优化算法-梯度下降算法"}},[_._v("#")]),_._v(" 优化算法——梯度下降算法")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("搜索方式")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/fe5e35cc-3fa8-41b5-9bb6-2a28fcad6c52-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("学习率和下降方向的具体含义")]),_._v(" "),i("ul",[i("li",[_._v("学习率\n"),i("ul",[i("li",[_._v("含义\n"),i("ul",[i("li",[_._v("梯度下降的“步幅”")])])])])]),_._v(" "),i("li",[_._v("下降方向\n"),i("ul",[i("li",[_._v("含义\n"),i("ul",[i("li",[_._v("梯度下降的“方向”")])])])])])])]),_._v(" "),i("li",[i("p",[_._v("对于二次函数，学习率的范围是什么？")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3c24bd58-a3dd-473e-bcf0-7dd387262fb7-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"模型"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#模型"}},[_._v("#")]),_._v(" 模型")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("线性回归模型")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("定义")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/54b61b76-d367-477a-9b10-16906ddcb061-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("模型训练")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a94f9555-9ab2-4837-8ccf-07e2b936b890-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("平方误差损失函数")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/761fe4c5-6649-4b12-9841-3858db21e662-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/ca80fb57-e0a3-429a-a2f2-6ea841efa320-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("特点")]),_._v(" "),i("ul",[i("li",[_._v("一般用作二分类，也可以将多个二分类器组合使用，实现多分类")])])]),_._v(" "),i("li",[i("p",[_._v("使用梯度下降算法求解线性回归模型")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/c3e5ba85-d485-4645-9feb-7972fd70238c-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("Softmax模型")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("作用")]),_._v(" "),i("ul",[i("li",[_._v("实现多分类")])])]),_._v(" "),i("li",[i("p",[_._v("one-hot编码")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("定义")]),_._v(" "),i("ul",[i("li",[_._v("用N位状态寄存器来对N个状态进行编码。第i位是1其余位为0，则说明其属于第i类")])])]),_._v(" "),i("li",[i("p",[_._v("举个栗子🌰")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3889751d-9789-4358-91c4-f2e1e5eb637f-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/e0f127f2-9d0c-4396-a299-6e62d1315dce-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("输出特点")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/fe28bf34-2e66-424e-85c8-2ec8a72178b7-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("前向计算")]),_._v(" "),i("ul",[i("li",[_._v("待续")])])]),_._v(" "),i("li",[i("p",[_._v("交叉熵(cross-entropy )损失函数")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("定义")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/10c5e302-42b6-43c3-a40b-9b1544c674dd-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("计算")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("举个栗子🌰")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/14d813af-2216-4f29-8e9d-abc07fafd6ea-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("PyTorch中的交叉熵损失函数")]),_._v(" "),i("ul",[i("li",[_._v("名称与使用\n"),i("ul",[i("li",[_._v("from torch import nn")]),_._v(" "),i("li",[_._v("loss = nn.CrossEntropyLoss()")])])])])])])])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"多层神经网络-多层感知机"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#多层神经网络-多层感知机"}},[_._v("#")]),_._v(" 多层神经网络（多层感知机）")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("生物学启示")]),_._v(" "),i("ul",[i("li",[_._v("生物神经元\n"),i("ul",[i("li",[_._v("构成\n"),i("ul",[i("li",[_._v("树突、胞体和轴突")]),_._v(" "),i("li",[_._v("突触\n"),i("ul",[i("li",[_._v("一个神经元和轴突另一个神经元的树突连接点")])])])])]),_._v(" "),i("li",[_._v("由神经元结构和以及由化学过程决定的突触的连接强度，建立起神经网络的功能")])])])])]),_._v(" "),i("li",[i("p",[_._v("常见的传输（激活）函数有哪些？")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("sigmoid函数")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/90914859-636c-4ed6-9c9e-a0e3546a0266-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("ReLU函数")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/4fcd56c8-25e7-4608-aada-0e08a9e7f932-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("tanh函数")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/acf0ff36-53b5-4472-abce-269f791895b2-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("输入与输出的关系")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3ad85426-8f92-4f25-b782-d8f8487e18f5-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[i("p",[_._v("举个栗子🌰")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/d35ab2bd-fe27-4f72-830e-0c60c244fc1d-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("反向传播算法")]),_._v(" "),i("ul",[i("li",[_._v("计算\n"),i("ul",[i("li",[i("p",[_._v("步骤")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/0c21eaab-ae94-42fe-a23c-ba74b3f80fc2-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("举个栗子🌰")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("问：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a02c90a1-bb45-4242-ab70-7acad51b94c1-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("解：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/1afecbe2-400a-4efc-9a19-741a983c8263-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f28471fc-e558-46b7-a862-eb1cfc62a7a4-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("如果损失函数为平方误差以及激活函数为sigmoid呢？")])])])]),_._v(" "),i("li",[_._v("缺点\n"),i("ul",[i("li",[_._v("（1）学习率的设置比较困难")]),_._v(" "),i("li",[_._v("（2）学习率合适的情况下收敛速度较慢")]),_._v(" "),i("li",[_._v("（3）很有可能收敛到局部极小值点")]),_._v(" "),i("li",[_._v("（4）如果传输函数是sigmoid函数或类似函数，会导致梯度消失，导致网络未充分训练")])])])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"pytorch基础"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#pytorch基础"}},[_._v("#")]),_._v(" PyTorch基础")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("什么是Pytorch？")])]),_._v(" "),i("li",[i("p",[_._v("张量（tensor）的概念")]),_._v(" "),i("ul",[i("li",[_._v("使用前先import tensor")])])]),_._v(" "),i("li",[i("p",[_._v("tensor的创建")])]),_._v(" "),i("li",[i("p",[_._v("tensor的操作和运算，特别是矩阵运算")])]),_._v(" "),i("li",[i("p",[_._v("自动求梯度autograd包")]),_._v(" "),i("ul",[i("li",[_._v("根据输入和前向传播过程自动构建计算图，执行反向传播，计算梯度。Tensor的核心类，如果将其属性.requires_grad设置为True，它将开始追踪在其上的所有操作，就可以利用链式法则进行梯度传播了。完成计算后，可以调用.backward()来完成所有梯度计算。此Tensor的梯度将累积到.grad属性中。")])])]),_._v(" "),i("li",[i("p",[_._v("PyTorch核心包与其他包")]),_._v(" "),i("ul",[i("li",[_._v("torch.nn（核心包）\n"),i("ul",[i("li",[i("p",[_._v("说明")]),_._v(" "),i("ul",[i("li",[_._v("nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。常见的做法是继承nn.Module，撰写自己的网络/层。nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法")])])]),_._v(" "),i("li",[i("p",[_._v("代码")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/bea92d0c-4350-4205-a01d-f745aa1847c9-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("其他预定类")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("Sequential类")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a65a09e4-7ba6-4c83-bf88-1af95d9d5467-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("ModuleDict类")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/38a9a126-613b-47c1-9fc9-5a226c929163-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("不同模型类访问参数的方式")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("parameters()函数")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/805cd96b-2f17-497a-9954-ff85388fa3e7-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("named_parameters()函数")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f221d00a-783e-45fa-92b6-e2b7af67d1f2-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("模型参数的持久化")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("torch.save(t,PATH)/torch.load(PATH)")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/ad4990a5-ade4-4797-a2fd-05860ee63346-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/569bce81-b5bc-4eda-b5c6-574140374a29-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[_._v("举个栗子🌰\n"),i("ul",[i("li",[i("p",[_._v("问：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/85ca9db9-068f-45d4-81d0-4829c4710494-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("解：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/8b978710-00c0-45d4-90c6-b720db7a6c4f-7720513.jpg",alt:""}})])])])])])]),_._v(" "),i("li",[i("p",[_._v("model.state_dict()与model.load_state_dict(stdict)")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f3995c3c-b584-4081-b183-f65db951c222-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[_._v("model.state_dict()：获取模型状态")]),_._v(" "),i("li",[_._v("model.load_state_dict(stdict)：将模型状态加载到模型中")]),_._v(" "),i("li",[_._v("举个栗子🌰\n"),i("ul",[i("li",[i("p",[_._v("问：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/823a3bae-6256-46a3-9c97-b79bd92a887e-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("解：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/fc37698d-c5d3-435c-8167-ff7a6639e363-7720513.jpg",alt:""}})])])])])])])])]),_._v(" "),i("li",[i("p",[_._v("模型参数类型的转换")]),_._v(" "),i("ul",[i("li",[_._v("net = net.to(device)，转换到有device指定的计算单元上，CPU 或者GPU。")])])]),_._v(" "),i("li",[i("p",[_._v("预定义层")]),_._v(" "),i("ul",[i("li",[_._v("i. 2d卷积层:Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,...)")]),_._v(" "),i("li",[_._v("ii. 最大池化层:MaxPool2d(kernel_size, stride=None, padding=0,...)")]),_._v(" "),i("li",[_._v("iii. 线性层：Linear(in_features, out_features, bias=True)")]),_._v(" "),i("li",[_._v("iv. 均方误差损失函数：MSELoss(size_average=None, reduce=None, reduction='mean')")]),_._v(" "),i("li",[_._v("v. 交叉熵损失函数：CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')")]),_._v(" "),i("li",[_._v("vi. 丢弃层：Dropout(p=0.5, inplace=False)")]),_._v(" "),i("li",[_._v("vii. 归一化层：BatchNorm2d(num_features, ...)，或者BatchNorm1d")]),_._v(" "),i("li",[_._v("viii. 非线性激活函数层：包括sigmoid，ReLU等函数")])])])])]),_._v(" "),i("li",[_._v("torch.utils.data\n"),i("ul",[i("li",[_._v("主要函数\n"),i("ul",[i("li",[_._v("DataLoader(dataset, batch_size=1, shuffle=False , num_workers=0,...)")]),_._v(" "),i("li",[_._v("用途：将map风格的dataset转换为多个batch，返回值得每个元素是一个batch")])])])])])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"神经网络的一些概念"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#神经网络的一些概念"}},[_._v("#")]),_._v(" 神经网络的一些概念")]),_._v(" "),i("ul",[i("li",[_._v("（1）训练误差（training error）\n"),i("ul",[i("li",[_._v("指模型在训练数据集上表现出的误差。")])])]),_._v(" "),i("li",[_._v("（2）泛化误差（generalization error）\n"),i("ul",[i("li",[_._v("指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。")]),_._v(" "),i("li",[_._v("计算训练误差和泛化误差可以使用损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。")])])]),_._v(" "),i("li",[_._v("（3）欠拟合（underfitting）\n"),i("ul",[i("li",[_._v("模型无法得到较低的训练误差")])])]),_._v(" "),i("li",[_._v("（4）过拟合（overfitting）\n"),i("ul",[i("li",[_._v("概念\n"),i("ul",[i("li",[_._v("模型的训练误差远小于它在测试数据集上的误差。")])])]),_._v(" "),i("li",[_._v("解决过拟合的几种方法\n"),i("ul",[i("li",[_._v("（1）权重衰减（weight decay）\n"),i("ul",[i("li",[_._v("权重衰减等价于L2范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。L2范数正则化在模型原损失函数基础上添加L2范数惩罚项，从而得到训练所需要最小化的函数。L2范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，使得学出的参数值较小接近于零，这可能对过拟合有效。")])])]),_._v(" "),i("li",[_._v("（2）丢弃法（dropout）\n"),i("ul",[i("li",[_._v("在训练每次迭代是，某些神经元按指定概率丢弃，从而在训练模型时起到正则化的作用，可以用来应对过拟合。在测试模型时，为了拿到更加确定性的结果，一般不使用丢弃法。")])])])])])])]),_._v(" "),i("li",[_._v("（5）模型复杂度对训练的影响\n"),i("ul",[i("li",[_._v("给定数据集下，模型越简单更可能会欠拟合，越复杂更可能会过拟合。")])])]),_._v(" "),i("li",[_._v("（6）数据集大小对训练的影响\n"),i("ul",[i("li",[_._v("给定模型复杂度的情况下，数据集越小越容易过拟合，数据集越大，在计算能力不足的情况下容易欠拟合。")])])]),_._v(" "),i("li",[_._v("（7）迭代次数对训练得影响\n"),i("ul",[i("li",[_._v("迭代次数过多容易过拟合，过少容易欠拟合。")])])]),_._v(" "),i("li",[_._v("（8）梯度消失\n"),i("ul",[i("li",[_._v("在反向传播过程中需要乘上激活函数的导数，如果激活函数是sigmoid、tanh等函数类似时（输入较大或较小是其斜率趋向于零），在参数初始化不当或有分布特殊的情况下，会导致这些函数的输入值偏大而使其导数接近于零，多层传递后梯度越来越接近于零，从而使底层的训练无效，不论学习如何调整训练都没有进展，也无法加深网络。")])])]),_._v(" "),i("li",[_._v("（9）梯度爆炸\n"),i("ul",[i("li",[_._v("激活函数是类似ReLU函数时，如果参数初始化不当或者分布特殊，由于其导数在大于0是恒为1，则多层求和传递求后，梯度值会变得非常大，可能会超出计算机所能表示的范围。梯度爆炸会导致对学习率敏感，不够小的学习率会导致更大的梯度，太小的学习率训练又没进展，在学习期间可能要大幅调整学习率。")])])]),_._v(" "),i("li",[_._v("（10）数值稳定性的办法（解决梯度消失和梯度爆炸的问题）：\n"),i("ul",[i("li",[_._v("a) 选择合适的激活函数：对sigmoid和ReLU做适当的修改")]),_._v(" "),i("li",[_._v("b) 适当的初始化权重参数，比如使用Xavier初始化和Kaiming初始化。它们分别针对什么激活函数？Kaiming:Relu.")]),_._v(" "),i("li",[_._v("c) 归一化，包括数据集的归一化和每一层输出的归一化。归一化的目的是使数据现在在标准值之内。")]),_._v(" "),i("li",[_._v("d) 改变网络结构，如果残差网络ResNet等。")])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"优化算法与学习模型"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#优化算法与学习模型"}},[_._v("#")]),_._v(" 优化算法与学习模型")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("常见的优化算法")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("（1）什么是动量法，其公式是什么？相比梯度下降有什么优点？")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("动量法")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a99d03a8-34e2-472b-a522-aebee0ccebe2-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("公式")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/c0c73a3b-a525-4288-a22c-c74c9b201d90-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("优点")]),_._v(" "),i("ul",[i("li",[_._v("加速学习")]),_._v(" "),i("li",[_._v("当我们使用动量优化算法的时候，可以解决小批量SGD优化算法更新幅度摆动大的问题，同时可以使得网络的收敛速度更快。")])])])])]),_._v(" "),i("li",[i("p",[_._v("（2）使用公式描述Adagrad算法，它有什么优缺点？")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("公式")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a2516206-af70-4372-8b8a-848b79d5e080-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("优点")]),_._v(" "),i("ul",[i("li",[_._v("根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，避免统一的学习率难以适应所有维度的问题。")])])]),_._v(" "),i("li",[i("p",[_._v("缺点")]),_._v(" "),i("ul",[i("li",[_._v("当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解")]),_._v(" "),i("li",[_._v("存在提早结束问题")])])])])]),_._v(" "),i("li",[i("p",[_._v("（3）使用公式描述RMSProp算法，它有什么特点？")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("公式")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/24f91299-c54b-4082-ba96-5550445f8fbf-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("特点")]),_._v(" "),i("ul",[i("li",[_._v("RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。")])])])])]),_._v(" "),i("li",[i("p",[_._v("（4）使用公式描述AdaDelta算法，它有什么特点？")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("公式")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/9823ce9b-bc3f-4f0b-8843-245ce2052df3-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("特点")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/6f40430b-83aa-432c-af06-17c00116d013-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("（5）使用公式描述Adam算法，它有什么特点？")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("公式")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f17560be-ad23-4238-9b17-4c41a719f736-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("特点")]),_._v(" "),i("ul",[i("li",[_._v("AdaDelta算法针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进。")]),_._v(" "),i("li",[_._v("RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均")]),_._v(" "),i("li",[_._v("Adam算法使用了偏差修正")]),_._v(" "),i("li",[_._v("是RMSProp算法与动量法的结合")])])])])]),_._v(" "),i("li",[i("p",[_._v("（6）比较它们的收敛速度和收敛质量。")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/c09785b2-f7c5-4554-9a31-3102e5e95aaf-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("优化算法与学习模型的区别")]),_._v(" "),i("ul",[i("li",[_._v("（1）优化算法是在训练集上搜索极值点，目标是降低训练误差。")]),_._v(" "),i("li",[_._v("（2）学习模型的的过程是在训练集上降低训练误差，并尽量避免欠拟合和过拟合，目标是减低泛化误差。")])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"卷积神经网络"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#卷积神经网络"}},[_._v("#")]),_._v(" 卷积神经网络")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("视觉基本机制")]),_._v(" "),i("ul",[i("li",[_._v("每个神经元有它的各自的局部感受野，一个神经元有局部感受野，即一个神经元只连接部分输入，平铺或重叠构成整个图像；图像的特征逐层提高其抽象层次。")])])]),_._v(" "),i("li",[i("p",[_._v("卷积核")]),_._v(" "),i("ul",[i("li",[_._v("概念\n"),i("ul",[i("li",[_._v("一个二维数组，对应一个视觉神经元感受野区域内的权值。")])])])])]),_._v(" "),i("li",[i("p",[_._v("卷积运算（互相关计算）")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("步骤")]),_._v(" "),i("ul",[i("li",[_._v("卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。")]),_._v(" "),i("li",[_._v("当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。")])])]),_._v(" "),i("li",[i("p",[_._v("图解")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/026ca96b-55a9-469a-badc-3cff872fc3dc-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/fc6de880-9e00-4949-9e73-cc1a380e58e8-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("填充（padding）")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/ae2beaab-12e1-429d-b354-b1cfc2b37fc9-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[_._v("举个栗子🌰\n"),i("ul",[i("li",[_._v("问：卷积核的大小为3*3，填充参数为1，求填充后的输出窗口大小")]),_._v(" "),i("li",[_._v("解：在卷积周围填充一层padding，那么输出窗口大小为3+1*2=5")])])])])]),_._v(" "),i("li",[i("p",[_._v("步幅（stride）")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("概念")]),_._v(" "),i("ul",[i("li",[_._v("将每次滑动的行数和列数称为步幅（stride）")])])]),_._v(" "),i("li",[i("p",[_._v("图解")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f92b8d56-8eb1-4e86-a960-593a9d3ce888-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("通道（channel）")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("概念")]),_._v(" "),i("ul",[i("li",[_._v("彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是 ℎ和 𝑤（像素），那么它可以表示为一个 3×ℎ×𝑤的多维数组。我们将大小为3的这一维称为通道（channel）维。")])])]),_._v(" "),i("li",[i("p",[_._v("图解")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/ad1948ef-0e84-4706-bc7b-009a6c8c6056-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/e03f3c6f-20e6-4e48-bf86-dca4a86f202b-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/2ea1e3cc-e2b3-4317-867c-3e0a3d262251-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a400a676-7586-42bb-b9bf-c0f362d03d2d-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/4c4e2a98-b760-413b-856c-491a9f524d0c-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a714591b-5f2d-4572-9488-5ed79ee88d29-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("池化（pooling）")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("目的")]),_._v(" "),i("ul",[i("li",[_._v("特征不变性、降维，防止过拟合。")])])]),_._v(" "),i("li",[i("p",[_._v("概念与池化步骤")]),_._v(" "),i("ul",[i("li",[_._v("池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出")]),_._v(" "),i("li",[_._v("池化层直接计算池化窗口内元素的最大值或者平均值")]),_._v(" "),i("li",[_._v("池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动")])])]),_._v(" "),i("li",[i("p",[_._v("图解")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/02953b87-fe91-4ac6-a7f9-2e2b7edbeb6b-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("输出层大小的计算公式")]),_._v(" "),i("ul",[i("li",[_._v("（（输入+（填充）*2）-卷积核+步幅）/步幅")])])]),_._v(" "),i("li",[i("p",[_._v("综合计算")]),_._v(" "),i("ul",[i("li",[_._v("举栗子🌰\n"),i("ul",[i("li",[i("p",[_._v("输出层大小的计算")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("问：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/0321a846-a9e4-4136-bdd0-bb75ff20d6e6-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("解：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/64909341-44a8-43e6-a89f-17c89a5bbd4c-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("参数的多少的计算")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("问：")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f660d491-a1ab-4e7e-bec8-3b31b0119d70-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("解：所有参数个数bias+weight=（卷积核的大小3*3+偏置1（默认都是1））*2*3=60；bias=（偏置1）*2*3=6；weight=（卷积核的大小3*3）*2*3=54；")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/e2c14dab-6d57-44e0-899b-2fe6ed88fa89-7720513.jpg",alt:""}})])])])])])])])]),_._v(" "),i("li",[i("p",[_._v("LeNet")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("第一个卷积神经网络")])]),_._v(" "),i("li",[i("p",[_._v("架构")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/441648f4-f493-463a-839f-bac687c5edd2-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("代码")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/a6706e34-7755-4bf6-b063-cfa2c9121f57-7720513.jpg",alt:""}})])])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"深度卷积网络"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#深度卷积网络"}},[_._v("#")]),_._v(" 深度卷积网络")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("AlexNet——更深更大的 LeNet")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("架构")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/841fba9d-3304-4223-87bc-d8930f6b72d3-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/09d48883-0dc8-4755-aa4b-b9bd63879f95-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/42a75a12-7e16-46f9-a34b-a836fa60f5b3-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("相比于LeNet它采用哪些新的方法")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/799bb6a0-2300-45d0-b267-6781c86a0ec7-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[_._v("丢弃法（正则化）\n"),i("ul",[i("li",[_._v("在两个隐含层之后应用丢弃法（更好的稳定性 / 正则化）")])])]),_._v(" "),i("li",[_._v("ReLu 激活函数（训练）\n"),i("ul",[i("li",[_._v("将激活函数从 sigmoid 更改为 ReLu（不再梯度消失）")])])]),_._v(" "),i("li",[_._v("最大池化法")]),_._v(" "),i("li",[_._v("数据增强")])])])])]),_._v(" "),i("li",[i("p",[_._v("VGG——升华版的 AlexNet、重复的 VGG 块")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("VGG块")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/488bc05c-915b-47d9-a110-eb5d323e15e0-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("结构")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/25341b79-7807-440b-a494-54c9c822f4cb-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("变化")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/041622f4-41c9-4bdd-b0cf-7070018625c1-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[_._v("更大更深的 AlexNet (重复的 VGG 块)")])])])])]),_._v(" "),i("li",[i("p",[_._v("NiN——1x1 卷积 +使用全局池化层替代稠密层")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("1X1的卷积层的作用是什么？")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("1*1卷积过滤器 ,它的大小是1*1，没有考虑在前一层局部信息之间的关系。最早出现在 Network In Network的论文中 ，使用1*1卷积是想加深加宽网络结构 ，在Inception网络（ Going Deeper with Convolutions ）中用来降维。")])]),_._v(" "),i("li",[i("p",[_._v("由于3*3卷积或者5*5卷积在几百个filter的卷积层上做卷积操作时相当耗时，所以1*1卷积在3*3卷积或者5*5卷积计算之前先降低维度。")])]),_._v(" "),i("li",[i("p",[_._v("结论")]),_._v(" "),i("ul",[i("li",[_._v("1、降维（ dimension reductionality ）。比如，一张500 * 500且厚度depth为100 的图片在20个filter上做1*1的卷积，那么结果的大小为500*500*20")]),_._v(" "),i("li",[_._v("2、加入非线性。卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；")])])]),_._v(" "),i("li",[i("p",[_._v("ppt")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f82f1cf2-452b-46f8-adc4-83b026ecff4b-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("主要贡献")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/bd45de47-2fc1-4f3b-bc5c-17e4fbd23e4c-7720513.jpg",alt:""}})]),_._v(" "),i("ul",[i("li",[i("p",[_._v("打破最后一层稠密层")]),_._v(" "),i("ul",[i("li",[_._v("稠密层导致参数大量增加，增大计算量")])])]),_._v(" "),i("li",[i("p",[_._v("NiN块")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/8760e0fa-8f35-409e-84e4-0f9bf9b398b7-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("NiN最后一层")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f1c277ba-3950-47a0-817f-cc88ae40202d-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("总结")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("逐步降低图像分辨率")])]),_._v(" "),i("li",[i("p",[_._v("增加通道数量")])]),_._v(" "),i("li",[i("p",[_._v("应用全局平均池化")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/b4ae6957-4265-4a04-9230-38a9e09477b1-7720513.jpg",alt:""}})])])])])])])])]),_._v(" "),i("li",[i("p",[_._v("GoogLeNet")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("GoogLeNet提出了一个什么结构？")]),_._v(" "),i("ul",[i("li",[_._v("Inception\n"),i("ul",[i("li",[i("p",[_._v("做了什么？")]),_._v(" "),i("ul",[i("li",[_._v("卷积的不均匀混合（不同深度）")]),_._v(" "),i("li",[_._v("批量归一正则化")])])]),_._v(" "),i("li",[i("p",[_._v("作用")]),_._v(" "),i("ul",[i("li",[_._v("增加网络深度和宽度的同时减少参数。")])])]),_._v(" "),i("li",[i("p",[_._v("标准的Inception块")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3d3b933b-9bb9-4b9d-9417-55a043108ff4-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("图解")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/634e7cd7-6849-4f5b-8f1a-388886dd5363-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/7623f1d9-ab7c-4338-b4dc-ae44ae4c35c3-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/fcd32cf7-b49d-4831-b692-e475f48048e5-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/9eb64033-ceae-48e9-bd26-fc15b36231e6-7720513.jpg",alt:""}})])])])])])]),_._v(" "),i("li",[i("p",[_._v("结构")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/49d7d900-abc9-479e-8f1f-e834bd80e508-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/d854e67b-0d40-479b-8d9a-9215ce9df9f6-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/0304eaee-4423-44b7-971f-5453b6edbea1-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/4a026931-aaa3-4a58-87d7-f0bfc2d31ee5-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("残差网络（ResNet）")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("残差块")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("结构")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/b058d0a6-71b3-4a9a-9024-d2fa80de1a48-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/0c23ea4b-8a41-417c-9924-3b42fc7493fa-7720513.jpg",alt:""}})]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/ef22563d-54da-4dae-81ee-b198d9e24f25-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("结构")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/e0143d20-e5d4-47fb-aa35-066400f85207-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("主要作用")]),_._v(" "),i("ul",[i("li",[_._v("解决深度神经网络的退化问题")])])]),_._v(" "),i("li",[i("p",[_._v("它突破了哪个限制？")]),_._v(" "),i("ul",[i("li",[_._v("网络层数限制")])])])])])])]),_._v(" "),i("li",[i("h2",{attrs:{id:"图像增广与微调"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#图像增广与微调"}},[_._v("#")]),_._v(" 图像增广与微调")]),_._v(" "),i("ul",[i("li",[_._v("增广\n"),i("ul",[i("li",[i("p",[_._v("方式")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("反转")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/5e3dc497-a93c-4d63-9411-9b82eaf6e114-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("裁剪")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/f1c1ac21-9dc4-4f94-b164-4a10f256fe09-7720513.jpg",alt:""}})])]),_._v(" "),i("li",[i("p",[_._v("变色")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/6cede2ea-95d0-4ef7-9926-cb8893e855b8-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("作用")]),_._v(" "),i("ul",[i("li",[_._v("随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力")])])])])]),_._v(" "),i("li",[_._v("微调\n"),i("ul",[i("li",[_._v("基本方法与注意事项\n"),i("ul",[i("li",[i("p",[_._v("微调中的权重初始化")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("对输出层之前的进行微调，最后一层从头训练")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/3a35c357-739b-4920-abb6-43d04286e2a8-7720513.jpg",alt:""}})])])])]),_._v(" "),i("li",[i("p",[_._v("微调训练")]),_._v(" "),i("ul",[i("li",[_._v("在已有神经网络上微调目标数据集，但具有强大的正规化\n"),i("ul",[i("li",[_._v("使用较小的学习率")]),_._v(" "),i("li",[_._v("使用较少的迭代周期")])])]),_._v(" "),i("li",[_._v("如果源数据集比目标数据集更复杂，则微调通常会得到更高质量的模型")])])]),_._v(" "),i("li",[i("p",[_._v("重用已有分类器的参数")]),_._v(" "),i("ul",[i("li",[_._v("源数据集可能包含目标数据集中的某些类别")]),_._v(" "),i("li",[_._v("在初始化期间使用来自预训练模型的相应权重向量")])])]),_._v(" "),i("li",[i("p",[_._v("修复一些层")]),_._v(" "),i("ul",[i("li",[i("p",[_._v("神经网络学习分层特征表示")]),_._v(" "),i("ul",[i("li",[_._v("低层特征是通用的")]),_._v(" "),i("li",[_._v("高层特征与数据集中的对象更相关")])])]),_._v(" "),i("li",[i("p",[_._v("在微调期间修复底层的参数")]),_._v(" "),i("ul",[i("li",[_._v("在微调期间修复底层的参数")])])]),_._v(" "),i("li",[i("p",[_._v("图")]),_._v(" "),i("p",[i("img",{attrs:{src:"https://img.mubu.com/document_image/440a2c47-3e19-4576-8e56-db9a90921543-7720513.jpg",alt:""}})])])])])])])])])])])]),_._v(" "),i("p",[i("a",{attrs:{href:"https://mubu.com/doc/explore/34397",target:"_blank",rel:"noopener noreferrer"}},[_._v("本人幕布链接"),i("OutboundLink")],1)])])}),[],!1,null,null,null);t.default=a.exports}}]);